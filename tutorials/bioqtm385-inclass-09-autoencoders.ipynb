{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bioqtm385-inclass-09-autoencoders.ipynb","provenance":[{"file_id":"1wXqypWCeEStzCGEN6Y3KE1XpXzJltTQT","timestamp":1636319412079}],"authorship_tag":"ABX9TyNSfzeme+RXBHmUm/LQS08q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"vHwI4KVUQse8"},"source":["##**BIO/QTM 385: In class exercise for Wednesday, November 10th** \n","\n","\n","(answers will be the part of Assignment #5, Due 11/17)"]},{"cell_type":"markdown","metadata":{"id":"3db7A-x7Q47b"},"source":["<font color='green'>**Enter your names and group number here.**  </font>"]},{"cell_type":"markdown","metadata":{"id":"r17JLJFhQ8IZ"},"source":["*This notebook contains modified excerpts from the [Hands-One Machine Learning with Scikit-Learn, Keras & TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) by Aurelien Geron; the content is available [on GitHub](https://github.com/ageron/handson-ml2). Note that the full version of this text book is available online through the Emory library.  The code is released under the [Apache-2.0 License](https://github.com/ageron/handson-ml2/blob/master/LICENSE).  In addition, aspects have been adapted from the Neuromatch course materials [here](https://github.com/NeuromatchAcademy/course-content/blob/master/tutorials/README.md).*"]},{"cell_type":"markdown","metadata":{"id":"5AHRKsrwSq90"},"source":["As always, all questions to be answered will be in <font color=\"blue\"> blue</font> and places to write your answers will be in <font color=\"green\"> green</font>."]},{"cell_type":"markdown","metadata":{"id":"OzymHhYFSvgP"},"source":["To start, make sure that you go up to the **Runtime** menu, and make sure that *Hardware Accelerator* is set to *GPU* (otherwise your code will take forever to run)"]},{"cell_type":"code","metadata":{"id":"Pcnw1jzMQoTl"},"source":["!nvidia-smi;\n","import numpy as np\n","import os\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import keras\n","import keras.layers\n","from keras.layers import Dense, Input, Flatten, Reshape\n","from keras.layers import GaussianNoise, Dropout\n","from keras.models import Model, Sequential\n","from tensorflow.keras.utils import to_categorical\n","!pip install umap-learn[plot]\n","from umap import UMAP"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VFvZqhm-TYMJ","cellView":"form"},"source":["# @title Helper Functions & Figure Settings\n","\n","import ipywidgets as widgets       # interactive display\n","%config InlineBackend.figure_format = 'retina'\n","plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/NMA2020/nma.mplstyle\")\n","\n","mpl.rc('axes', labelsize=14)\n","mpl.rc('xtick', labelsize=12)\n","mpl.rc('ytick', labelsize=12)\n","\n","def plot_image(image):\n","    plt.imshow(image, cmap=\"binary\")\n","    plt.axis(\"off\")\n","\n","def plot_MNIST_sample(X):\n","  \"\"\"\n","  Plots 9 images in the MNIST dataset.\n","\n","  Args:\n","     X (numpy array of floats) : Data matrix each column corresponds to a\n","                                 different random variable\n","\n","  Returns:\n","    Nothing.\n","\n","  \"\"\"\n","\n","  fig, ax = plt.subplots()\n","  k = 0\n","  for k1 in range(15):\n","    for k2 in range(15):\n","      k = k + 1\n","      plt.imshow(X[k],\n","                 extent=[(k1 + 1) * 28, k1 * 28, (k2+1) * 28, k2 * 28],\n","                 vmin=0, vmax=255,cmap='gray')\n","  plt.xlim((15 * 28, 0))\n","  plt.ylim((15 * 28, 0))\n","  plt.tick_params(axis='both', which='both', bottom=False, top=False,\n","                  labelbottom=False)\n","  plt.clim([0, 255])\n","  ax.set_xticks([])\n","  ax.set_yticks([])\n","  plt.show()\n","\n","\n","def plot_training_history(hist):\n","    fig, ax = plt.subplots(figsize=(12,6))\n","    ax.plot(hist.history['loss'], label='Training Loss', color='firebrick')\n","    ax.plot(hist.history['val_loss'], label='Validation Loss', color='royalblue')\n","    ax.legend(loc='center right')\n","    ax.set_ylabel(\"loss\")\n","    ax.set_xlabel(\"Epoch #\")    \n","    if 'val_accuracy' in hist.history.keys():\n","        ax2 = ax.twinx()\n","        ax2.plot(hist.history['accuracy'], '--', label='Training Accuracy', color='firebrick', )\n","        ax2.plot(hist.history['val_accuracy'], '--', label='Testing Accuracy', color='royalblue')\n","        ax2.legend(loc='lower left')\n","        ax2.set_ylabel(\"accuracy\")\n","\n","\n","\n","\n","def make_fashion_mnist_plot(X_valid_2D,X_valid,y_valid):\n","  # adapted from https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html\n","  cmap = plt.cm.tab10\n","  x = (X_valid_2D - X_valid_2D.min()) / (X_valid_2D.max() - X_valid_2D.min())\n","  plt.scatter(x[:, 0], x[:, 1], c=y_valid, s=10, cmap=cmap)\n","  image_positions = np.array([[1., 1.]])\n","  for index, position in enumerate(x):\n","    dist = np.sum((position - image_positions) ** 2, axis=1)\n","    if np.min(dist) > 0.02: # if far enough from other images\n","        image_positions = np.r_[image_positions, [position]]\n","        imagebox = mpl.offsetbox.AnnotationBbox(\n","            mpl.offsetbox.OffsetImage(X_valid[index], cmap=\"binary\"),\n","            position, bboxprops={\"edgecolor\": cmap(y_valid[index]), \"lw\": 2})\n","        plt.gca().add_artist(imagebox)\n","  plt.axis(\"off\")\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AvWzAdrQT5bV"},"source":["## Autoencoders and latent variables\n","How can simple algorithms capture relevant aspects of data and build robust models of the world?\n","\n","Autoencoders are a family of artificial neural networks (ANNs) that learn internal representations through auxiliary tasks, i.e., *learning by doing*.\n","\n","The primary task is to reconstruct output images based on a compressed representation of the inputs. This task teaches the network which details to throw away while still producing images that are similar to the inputs, as illustrated in the cartoon below.\n","\n","&nbsp;\n","\n","![MNIST cognitive task](https://github.com/mpbrigham/colaboratory-figures/raw/master/nma/autoencoders/mnist_task.png)\n","\n","&nbsp;\n","\n","The beauty of autoencoders is the possibility to see these internal representations. The bottleneck layer enforces data compression by having fewer units than input and output layers. Further limiting this layer to two or three units enables us to see how the autoencoder is organizing the data internally in two or three-dimensional **latent space** that provides a useful representation of our data. "]},{"cell_type":"markdown","metadata":{"id":"TOw0vDh3UhS2"},"source":["Mathematically, we can think of an autoencoder as two functions, an **encoder**, $f(\\vec{x})$, that transforms the input data $\\vec{x}$ into a low-dimensional representation, $\\vec{z}$, and a **decoder**, $g(\\vec{z})$, that takes the low-dimensional generated from the encoder, and attempts to reconstruct $\\vec{x}$.  In other words, the goal is to construct $f$ and $g$ such that $g(f(\\vec{x})) \\equiv \\vec{x}' = \\vec{x}$.  We think of $\\vec{z}$ as our **code** (or **latent variable**), that is, a compressed representation of our input data."]},{"cell_type":"markdown","metadata":{"id":"Zno3gv6Qpfet"},"source":["###Building an autoencoder in Keras"]},{"cell_type":"markdown","metadata":{"id":"7HDuWeIRpk0Q"},"source":["To build an autoencoder, it is often useful to use the ```Sequential``` API in Keras.  Networks are built in a manner similarly to what we saw in the previous notebook, but it allows us to define the encoder and the decoder individually (and it helps us more easily keep track of the layer dependencies as we create deeper networks).  \n","\n","For instance, let's say that we want to take a bunch of 28$\\times$28 pixel images (like MNIST), and create a stacked autoencoder where the first later of the encoder has 100 neurons and the middle layer has 30 neurons (the decoder is just the mirror of this).  We can define the autoencoder model via:"]},{"cell_type":"code","metadata":{"id":"hdKPtA_Xpkcs"},"source":["#define the encoder (flatten the image, then a 100 neuron layer, then a 30 neuron layer)\n","stacked_encoder = Sequential([\n","    Flatten(input_shape=[28, 28]),\n","    Dense(100, activation=\"selu\"),\n","    Dense(30, activation=\"selu\"),\n","])\n","\n","#define the decoder (a 100 neuron layer that takes an input of size 30, followed by a )\n","stacked_decoder = Sequential([\n","    Dense(100, activation=\"selu\", input_shape=[30]),\n","    Dense(28 * 28, activation=\"sigmoid\"),\n","    Reshape([28, 28])\n","])\n","\n","#create the model\n","stacked_ae = Sequential([stacked_encoder, stacked_decoder])\n","\n","#compile the model (using crossentropy as the loss function and the 'adam' optimizer)\n","stacked_ae.compile(loss=\"binary_crossentropy\",optimizer='adam', metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eEwdp8gSJymk"},"source":["Note how we are using the SELU (Scaled Expoential Linear Unit) here instead of ReLUs, tanh, etc.  This function is given by:\n","\\begin{equation}\n","\\phi(x) = \\left\\{\n","        \\begin{array}{ll}\n","            -\\lambda x & \\quad x \\leq 0 \\\\\n","            \\lambda\\alpha(e^x-1) & \\quad x \\geq 0\n","        \\end{array}\n","    \\right.\n","\\end{equation}\n","where $\\alpha$ = 1.67326324 and $\\lambda$ = 1.05070098 (it's complicated...).  It turns out that this function has much better properties than ReLUs, since it has a gradient everywhere (including below zero). "]},{"cell_type":"markdown","metadata":{"id":"TDfv9OJ7ugpY"},"source":["###An example: the Fashion MNIST Dataset\n","\n","As discussed in class, the Fashion MNIST data set is a collection of 28$\\times$28 pixel images of clothing, shoes, and other fashion-related objects.  For variety (there are only so many hand-written digits that one can look at), we will load this data set and will apply an autoencoder to these data."]},{"cell_type":"code","metadata":{"id":"c05oRUTvvdPZ"},"source":["#Load the Fashion MNIST Data, plot example images, and break into training, test, and validation sets\n","\n","(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data();\n","plot_MNIST_sample(X_train_full)\n","X_train_full = X_train_full.astype(np.float32) / 255; #this is to put all of the images between 0 & 1, which is often helpful\n","X_test = X_test.astype(np.float32) / 255;\n","X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:];\n","y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:];\n","\n","print('Training Set Shape = '+str(np.shape(X_train))+'\\n')\n","print('Validation Set Shape = '+str(np.shape(X_valid))+'\\n')\n","print('Test Set Shape = '+str(np.shape(X_test))+'\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8O-Bg8Wvwa_l"},"source":["Given the MNIST images that we have imported and the ```stacked_ae``` model defined in the previous section, we can now fit an autoencoder to these data (here using a batch size of 200 and training for 20 epochs)."]},{"cell_type":"code","metadata":{"id":"itIKk-5-xNKe"},"source":["#train the model for 20 epochs.  Note that we are trying to fit X_train to itself, using X_valid as the validation data\n","history = stacked_ae.fit(X_train, X_train, batch_size=200,epochs=20,validation_data=(X_valid, X_valid))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"47muz1SbzRp5"},"source":["#plot loss & accuracy\n","plot_training_history(history)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g1jEpOdWy0mX"},"source":["So how well did we do?  \n","\n","Let's generate a few example reconstructions using the code below (original images on the top, reconstructed ones on the bottom):"]},{"cell_type":"code","metadata":{"id":"uSwWpcWBzB2B"},"source":["def show_reconstructions(model, images=X_valid, n_images=10):\n","    reconstructions = model.predict(images[:n_images])\n","    fig = plt.figure(figsize=(n_images * 1.5, 3))\n","    for image_index in range(n_images):\n","        plt.subplot(2, n_images, 1 + image_index)\n","        plot_image(images[image_index])\n","        plt.subplot(2, n_images, 1 + n_images + image_index)\n","        plot_image(reconstructions[image_index])\n","\n","show_reconstructions(stacked_ae)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z5-Eoad20mh8"},"source":["Not bad, considering that all of the 784 pixels were crammed into only 30 neurons in the middle layer!\n","\n","So what is going on in this middle layer?  We can visualize this 30-dimensional space with UMAP."]},{"cell_type":"code","metadata":{"id":"1GkFOD-b1MGO"},"source":["#find the compressed representation (i.e., run the encoder forward without using the decoder)\n","X_valid_compressed = stacked_encoder.predict(X_valid)\n","\n","#apply UMAP\n","ae_umap = UMAP(n_components=2,n_neighbors=15,min_dist=.5)\n","Y = ae_umap.fit_transform(X_valid_compressed)\n","\n","#Make a pretty plot\n","make_fashion_mnist_plot(Y,X_valid,y_valid)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kNVAMti1448t"},"source":["Here, the points are colored by their label (e.g., 'shoe' or 't-shirt' or 'purse'), and a few examples are shown.  Notice how the autoencoder found this structure!"]},{"cell_type":"markdown","metadata":{"id":"Bp07x9eU5OAk"},"source":["<font color=\"blue\"> **Question #1**: Apply UMAP to the raw validation data (```X_valid```) and plot using the ```make_fashion_mnist_plot()``` function above.  Describe the similarities (if any) and differences (if any) between the representations.  In this case, did we learn anything from training the autoencoder? </font>"]},{"cell_type":"code","metadata":{"id":"IH-Q7fNl2lfT"},"source":["#Type your code for Question #1 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LNqr3J5s5sWS"},"source":["<font color=green> Describe the similarities (if any) and differences (if any) between the representations.  In this case, did we learn anything from training the autoencoder?</font>"]},{"cell_type":"markdown","metadata":{"id":"n06RipdA608U"},"source":["<font color=blue> **Question #2**: Build a new stacked autoencoder, ```stacked_ae_small``` that is the same as ```stacked_ae```, but has one more layer in the encoder that has only 2 neurons (and an extra layer in the decoder to expand it as well).  In other words, it should be $784\\to 100\\to 30 \\to 2 \\to 30 \\to 100 \\to 784$.  Train this network on the same data as above.  Plot the validation and training test and lost using ```plot_training_history()```.  Is there a noticible difference in test loss/accuracy between the networks?  (Note: (i) you might need to run it for 40-50 epochs in this case, (ii) sometimes due to initial condition problems, you might need to completely re-run the network if it doesn't train properly -- it should produce \"reasonable\" results, (iii) it might help to increase the batch size to ~500)</font>"]},{"cell_type":"code","metadata":{"id":"IBXAAi296ncp"},"source":["#Type your code for Question #2 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1WwU-FCN-ilG"},"source":["<font color=green> Is there a noticible difference in test loss/accuracy between the networks? </font>"]},{"cell_type":"markdown","metadata":{"id":"epiBj0vqBLxP"},"source":["<font color=blue> **Question #3**: Use the ```show_reconstructions()``` function (see above) to display a few example reconstructions from this new network.  Is the performance better, worse, or about the same (by eye)?"]},{"cell_type":"code","metadata":{"id":"YWK0NRmaBwBE"},"source":["#Type your code for Question #3 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7iwz7XWXBr8j"},"source":["<font color=green>Is the performance better, worse, or about the same (by eye)? </font>"]},{"cell_type":"markdown","metadata":{"id":"awCodfMdB-Ex"},"source":["<font color=blue>**Question #4**: Use ```make_fashion_mnist_plot()``` to plot the 2d representation from the smallest layer of the network (no need for UMAP this time, since it's already 2-dimensional).  Is the structure different than the structure derived from the previous autoencoder?  If so, describe why this would be a better or worse (or equivalent) representation.</font>"]},{"cell_type":"code","metadata":{"id":"XKSQhgcf9_4_"},"source":["#Type your code for Question #4 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P5vBwfd2Co32"},"source":["<font color=green> Is the structure different than the structure derived from the previous autoencoder?  If so, describe why this would be a better or worse (or equivalent) representation.</font>"]},{"cell_type":"markdown","metadata":{"id":"Eqh0LroZDR_J"},"source":["###Tying weights\n","\n","It is common to tie the weights of the encoder and the decoder, by simply using the transpose of the encoder's weights as the decoder weights. This speeds up training (fewer parameters) and reduces the risk of overfitting.\n","\n","For this, we need to use a custom layer in Keras, which we will call ```DenseTranspose```:"]},{"cell_type":"code","metadata":{"id":"4nc_qr3kDo0H"},"source":["class DenseTranspose(keras.layers.Layer):\n","    def __init__(self, dense, activation=None, **kwargs):\n","        self.dense = dense\n","        self.activation = keras.activations.get(activation)\n","        super().__init__(**kwargs)\n","    def build(self, batch_input_shape):\n","        self.biases = self.add_weight(name=\"bias\",\n","                                      shape=[self.dense.input_shape[-1]],\n","                                      initializer=\"zeros\")\n","        super().build(batch_input_shape)\n","    def call(self, inputs):\n","        z = tf.matmul(inputs, self.dense.weights[0], transpose_b=True)\n","        return self.activation(z + self.biases)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cij8rxaYDv2q"},"source":["Thus, given this new layer type, we can define an autoencoder ($784\\to 100\\to  30 \\to 100 \\to 784$) with tied weights via:"]},{"cell_type":"code","metadata":{"id":"YbrMXajID7U5"},"source":["dense_1 = Dense(100, activation=\"selu\")\n","dense_2 = Dense(30, activation=\"selu\")\n","\n","tied_encoder = Sequential([\n","    Flatten(input_shape=[28, 28]),\n","    dense_1,\n","    dense_2\n","])\n","\n","tied_decoder = Sequential([\n","    DenseTranspose(dense_2, activation=\"selu\"),\n","    DenseTranspose(dense_1, activation=\"sigmoid\"),\n","    Reshape([28, 28])\n","])\n","\n","tied_ae = Sequential([tied_encoder, tied_decoder])\n","\n","tied_ae.compile(loss=\"binary_crossentropy\",optimizer='adam', metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GaSa-pDuFogO"},"source":["<font color=blue>**Question #5**: Train ```tied_ae``` on the Fashion MNIST data for 10 epochs.  Compare the test accuracy and loss to the other two models that you've trained for 20 or more epochs. </font>"]},{"cell_type":"code","metadata":{"id":"6DUgFAdWEdB3"},"source":["#Type your code for Question #5 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ci18GuRQGAbr"},"source":["<font color=green>Compare the test accuracy and loss to the other two models that you've trained for 20 or more epochs. </font>"]},{"cell_type":"markdown","metadata":{"id":"3vVTBOxWGf7U"},"source":["##Denoising autoencoders"]},{"cell_type":"markdown","metadata":{"id":"JuqfnvOzIy3x"},"source":["A rather useful application of autoencoders is to remove noise from an image.  In other words, we will ask our encoder/decoder combination to return an image that can be less distorted than the one we started with!\n","\n","The essential idea here is that we will use the autoencoder to find a low-dimensional description of the data that ignores aspects that we explicitly characterize as noise.  This noise could be Gaussian noise that we add to input data, or it could be that we purposefully delete parts of the data (e.g., set a pixel value to zero), so that the network doesn't overtrain on a particular part of the image.  This process is called **Dropout**.\n","\n","If $X$ is our original data, $X'=g(f(X))$ is the autoencoder output, and $\\tilde{X}$ is our noise-corrupted data.  This new autoencoder will try to solve the equation \n","\\begin{equation}\n","X = g(f(\\tilde{X}))\n","\\end{equation}\n","as accurately as possible (with minimal loss).\n","\n","To create this network, we simply need to add an extra layer.  To add Gaussian noise, we can use ```GaussianNoise(sigma)``` (```sigma``` is the noise standard deviation), and to use Dropout, we can use ```Dropout(p)``` (```p``` is the probability of setting a given dimension/pixel to zero).\n","\n","For the case of adding Gaussian noise with $\\sigma = 0.2$, we use:"]},{"cell_type":"code","metadata":{"id":"bODqKcE8NRz1"},"source":["denoising_encoder = Sequential([\n","    Flatten(input_shape=[28, 28]),\n","    GaussianNoise(0.2),\n","    Dense(100, activation=\"selu\"),\n","    Dense(30, activation=\"selu\"),\n","    Dense(2, activation=\"selu\")\n","])\n","\n","denoising_decoder = Sequential([\n","    Dense(30, activation=\"selu\", input_shape=[2]),                                         \n","    Dense(100, activation=\"selu\"),                                         \n","    Dense(28 * 28, activation=\"sigmoid\"),\n","    Reshape([28, 28])\n","])\n","\n","\n","denoising_ae = Sequential([denoising_encoder, denoising_decoder])\n","denoising_ae.compile(loss=\"mse\", optimizer='adam',metrics=['accuracy'])\n","history_denoising = denoising_ae.fit(X_train, X_train, epochs=20,batch_size=100, validation_data=(X_valid, X_valid))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eLxctf5bNmRO"},"source":["**Note that for these types of networks, it is more common to get stuck in a bad initial condition, so if the loss/accuracy curves show little or no progress (see below), you should just run the code again.**"]},{"cell_type":"code","metadata":{"id":"6QH9SYNAGCIw"},"source":["plot_training_history(history_denoising)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LRYfK7OjSQdl"},"source":["Now let's see how it did!  We will input some noise to the images, and we'll see if the network was able to eliminate it."]},{"cell_type":"code","metadata":{"id":"lpv-24loQGRp"},"source":["noise = GaussianNoise(0.2)\n","show_reconstructions(denoising_ae, noise(X_valid, training=True))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tFs2gwTfSh90"},"source":["Not bad!  While the images are fuzzy (they've lost some definition), the gaussian pixel blur is gone."]},{"cell_type":"markdown","metadata":{"id":"kqe1fPxFSv_0"},"source":["<font color=blue> **Question #6**: Use the function above, but now with $\\sigma =3$.  Compare the outputs to the those above.  Describe why you think that you are you seeing this effect? (It might be helpful to use ```make_fashion_mnist_plot()``` on the post-encoder values for the non-noisy data set)"]},{"cell_type":"code","metadata":{"id":"Dup1HmlCSfvR"},"source":["#Type your code for Question #6 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B8UGcgcRUyTr"},"source":["<font color=green> Compare the outputs to the those above.  Describe why you think that you are you seeing this effect?"]},{"cell_type":"markdown","metadata":{"id":"VO8-PFguU4b7"},"source":["<font color=blue> **Question #7**: Train a similar network, but now using Dropout (use ```p``` = 0.5).  You should call the network ```dropout_ae```.  Remember to test the loss and accuracy curves to make sure that the network is actually training (and re-run if necessary).  "]},{"cell_type":"code","metadata":{"id":"ip4IPG5RU2AD"},"source":["#Type your code for Question #7 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RkMXFQ-hcMHI"},"source":["<font color=blue>**Question #8**: You can use: </font>\n","\n","<font color=blue> ```dropout = Dropout(p)``` </font>\n","\n","<font color=blue> ```show_reconstructions(dropout_ae, dropout(X_valid, training=True))``` </font>\n","\n","<font color=blue>to plot the examples of sending corrupted images through the autoencoder.  Plot these examples for various values of $p \\in [0.5,1)$. At what point do the images reconstruction begin to appear inaccurate?  Is the network doing better than you can to $p\\approx 0.9$?</font>\n"]},{"cell_type":"code","metadata":{"id":"Vehbae9mWM-q"},"source":["#Type your code for Question #8 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YLXPfApWeQF3"},"source":["<font color=green> At what point do the images reconstruction begin to appear inaccurate?  Is the network doing better than you can to $p\\approx 0.9$?"]},{"cell_type":"markdown","metadata":{"id":"wb8TjJ19dHyH"},"source":["<font color=\"blue\">**Question #9**: Apply ```make_fashion_mnist_plot()``` on the post-encoder values of the Dropout network.  Compare this plot to the plot you obtained from the non-denoising network in Question #4.  Explain why you think any differences might be occurring.</font>"]},{"cell_type":"code","metadata":{"id":"dFjAe5frb_zL"},"source":["#Type your code for Question #9 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HsLWx9QKeFtT"},"source":["<font color=green> Compare this plot to the plot you obtained from the non-denoising network in Question #4.  Explain why you think any differences might be occurring. </font>"]},{"cell_type":"markdown","metadata":{"id":"yw6PoGXMfmdV"},"source":["<font color=blue> **Question #10**: Reconstruct images corrupted with Dropout ($p=0.8$) using the Gaussian noise autoencoder, and reconstruct images corrupted with Gaussian noise using the Dropout autoencoder ($\\sigma=0.5$).  Describe what you see."]},{"cell_type":"code","metadata":{"id":"30nS3p5iedqY"},"source":["#Type your code for Question #10"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lLcQtWqkgLfq"},"source":["<font color=green> Describe what you see. </font>"]},{"cell_type":"markdown","metadata":{"id":"OXxG00cDOsMe"},"source":["<font color=blue> **Question #11**: By now, you should have seen a few examples of \"swapping\" (e.g., the input image is a high heel, but for some reason, a purse or a boot emerges from the network).  These networks all have the structure $784\\to 100\\to 30 \\to 2 \\to 30 \\to 100 \\to 784 $.  Would you expect to see more or less swapping if we used a network with structure $784\\to 100\\to 30 \\to 100 \\to 784 $?  Explain your answer qualitatively (no need to actually implement the network)."]},{"cell_type":"markdown","metadata":{"id":"FeSWfzPFPcKu"},"source":["<font color = green> Type your answer for Question #11 here </font>"]}]}