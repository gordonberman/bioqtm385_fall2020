{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bioqtm385-inclass-03-dimensionality-reduction.ipynb","provenance":[],"authorship_tag":"ABX9TyOnusg8PVVjYVd3u/Aex71w"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Yd7mdWvP9pI1"},"source":["##**BIO/QTM 385: In class exercise for Monday, September 20th** \n","\n","\n","(answers will be the part of Assignment #2, Due 9/29)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-iRIHK5l9vnr"},"source":["<font color='green'>**Enter your names and group number here.**  </font>"]},{"cell_type":"markdown","metadata":{"id":"gNtFftug9w8j"},"source":["*This notebook contains modified excerpts from the [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas; the content is available [on GitHub](https://github.com/jakevdp/PythonDataScienceHandbook). The text is released under the [CC-BY-NC-ND license](https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode), and code is released under the [MIT license](https://opensource.org/licenses/MIT).  In addition, aspects have been adapted from the Neuromatch course materials [here](https://github.com/NeuromatchAcademy/course-content/blob/master/tutorials/README.md).*"]},{"cell_type":"markdown","metadata":{"id":"LU5TlQxb-F_8"},"source":["For this exercise, we will explore a variety of Dimensionality Reduction methods on both simulated and real data.  As always, all questions to be answered will be in <font color=\"blue\"> blue</font> and places to write your answers will be in <font color=\"green\"> green</font>."]},{"cell_type":"code","metadata":{"id":"7tyci4-k9Z4N"},"source":["#import various useful packages\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import numpy.random as random\n","%matplotlib inline\n","\n","#importing dimensionality reduction packages\n","from sklearn.decomposition import PCA\n","from sklearn.decomposition import NMF\n","from sklearn.manifold import MDS\n","from sklearn.manifold import Isomap \n","from sklearn.manifold import LocallyLinearEmbedding\n","from sklearn.manifold import TSNE\n","!pip install umap-learn[plot]\n","import umap\n","\n","#importing clustering packages\n","from sklearn.cluster import KMeans\n","from sklearn.mixture import GaussianMixture"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RJN9mgDvSykC"},"source":["##Dimensionality Reduction Techniques in Python\n","\n","In this section, we'll introduce the general syntax for applying dimensionality reduction in python.  You'll notice that the syntax is similar across methods (and similar to ```PCA``` from last time), but there will be different choices for parameters.\n","\n","For all cases below, we'll assume that our data has the form $X\\in\\mathcal{R}^{N\\times d}$, where $N$ is the number of data points, and $d$ is the dimensionality of the dataset.  We will attempt to reduce the dimensionality of the data to create a dataset $Y\\in\\mathcal{R}^{N\\times m}$, where $m$ is the new dimensionality of the data.\n","\n","Note: Although there is no data provided for this part, ff you would like to test the syntax, just initialize an $N\\times d$ random matrix ```X``` and plug into the code below."]},{"cell_type":"markdown","metadata":{"id":"fpu8daRahQUo"},"source":["###Review: Principal Components Analysis (PCA)"]},{"cell_type":"markdown","metadata":{"id":"KF1Qd8uDm2nv"},"source":["As we've seen several tiems already, PCA performs dimensionality reduction by computing the eigenvectors and eigenvalues of $X$.\n","\n","Most of the dimensionality reduction techniques in python will use a very similar syntax."]},{"cell_type":"code","metadata":{"id":"yIIY3EhmhWov"},"source":["#To initialize a PCA class instance\n","pca_example = PCA() #Note: Use pca_example = PCA(n_components=m) to only compute for the largest m eigenmodes \n","\n","#To apply PCA to a data set\n","pca_example.fit(X)\n","\n","#To return back it's projections onto the eigenvectors\n","projections = pca_example.transform(X)\n","\n","#To apply PCA to a data set and simultaneously return back it's projections onto the eigenvectors\n","projections = pca_example.fit_transform(X)\n","\n","#The (sorted from largest to smallest) eigenvalues are in the class variable:\n","example_pca_eigenvalues = pca_example.explained_variance_\n","\n","#The fraction of the variance explained by each modes is given by:\n","example_pca_relative_variance = pca_example.explained_variance_ratio_"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rlMWf-9YTarN"},"source":["###Non-negative Matrix Factorization (NMF)"]},{"cell_type":"markdown","metadata":{"id":"vCNnHAygTeOg"},"source":["Similar to PCA, NMF is a linear decomposition, $X \\approx YH$, where $Y$ is our $N\\times m$ reduced-dimension coordinates and $W$ is our set of basis vectors.  What is different, though, is that (i) we're assuming that all elements of $X$, $Y$, and $H$ are $\\ge 0$ and (ii) we're no longer assuming that the basis vectors are orthogonal to each other.  Note: this method applies well to any data that has a hard upper or lower bound, even if that bound is not zero (we could add/subract the lower bound value from the data to make it zero, and we could multiply the data by $-1$ to turn an upper bound into a lower bound)."]},{"cell_type":"code","metadata":{"id":"5rkR4oVloW-E"},"source":["#Initializing the NMF object (note that there are other optiosn for regularizing/initializing)\n","nmf_example = NMF(n_components=2) \n","\n","#Fits the model and returns back the low-dimensional projections\n","Y = nmf_example.fit_transform(X)\n","\n","#To return the basis vectors\n","H = nmf_example.components_\n","\n","#Returns back the NMF error (the Frobenius mean of X - YH)\n","nmf_error = nmf_example.reconstruction_err_\n","\n","#To calculate the high-dimensional values of the low-d vectors\n","X_reconstructed = nmf_example.inverse_transform(Y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ofEtHfOOqorL"},"source":["###Multi-Dimensional Scaling (MDS)"]},{"cell_type":"markdown","metadata":{"id":"1bEI4XPfqs6K"},"source":["MDS minimizes the differences between distances in the high-dimensional space and the low dimensional space: $\\sum_{i,j} (D_{ij}(X) - \\Delta_{ij}(Y))^2$. "]},{"cell_type":"code","metadata":{"id":"i4PM6ttrrLe_"},"source":["#Initializing the MDS object.\n","mds_example = MDS(n_components=2) \n","\n","#Fits the model and returns back the low-dimensional projections\n","Y = mds_example.fit_transform(X)\n","\n","#If you wish to input a distance matrix (D in NxN) instead of the data matrix (say, if you want a non-Euclidean distance):\n","#mds_example = MDS(n_components=2,dissimilarity='precomputed')\n","#Y = mds_example.fit_transform(D)\n","\n","#To return the embedded points\n","Y2 = mds_example.embedding_\n","\n","#Returns back the MDS error (the cost function value at the found minimum)\n","#This is often useful for determining the value for n_components\n","mds_error = mds_example.stress_"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nDsBEGQagSPi"},"source":["###Isomap\n","\n","Isomap is effectively the same as MDS, but using a topological distance metric (as discussed in the reading and in class)."]},{"cell_type":"code","metadata":{"id":"ySolMr_KurI9"},"source":["#Initializing the Isomap object. n_components is the dimensionality of the embedding \n","#and n_neighbors is the number of neighbors used to construct the manifold.\n","#The 'metric' option can also be called if you want to choose a different distance metric\n","iso_example = Isomap(n_components=2,n_neighbors=20) \n","\n","#Fits the model and returns back the low-dimensional projections\n","Y = iso_example.fit_transform(X)\n","\n","#To return the embedded points\n","Y2 = iso_example.embedding_\n","\n","#Returns back the Isomap error (the cost function value at the found minimum)\n","#This is often useful for determining the value for n_components\n","iso_error = iso_example.reconstruction_error()\n","\n","#To return the geodesic distance matrix used by the algorithm\n","D_geodesic = iso_example.dist_matrix_"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yrIV8LVMgfpt"},"source":["###Local Linear Embedding (LLE)\n","\n","LLE is a local dimensionality reduction method that fits linear models to data in the high-dimensional space and then attempts to map the points into a lower-dimensional space in a manner such that these models are optimally preserved."]},{"cell_type":"code","metadata":{"id":"0Ex_A0G_JriF"},"source":["#Initializing the LLE object. n_components is the dimensionality of the embedding \n","#and n_neighbors is the number of neighbors used to construct the high-dimensional model.\n","LLE_example = LocallyLinearEmbedding(n_components=2,n_neighbors=20) \n","\n","#Fits the model and returns back the low-dimensional projections\n","Y = LLE_example.fit_transform(X)\n","\n","#To return the embedded points\n","Y2 = LLE_example.embedding_\n","\n","#Returns back the LLE error (the cost function value at the found minimum)\n","LLE_error = LLE_example.reconstruction_error_\n","\n","#Returns an object containing the nearest neighbotrs\n","neighbors_LLE = LLE_example.nbrs_"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8n7U0W2IgXtL"},"source":["###t-Distributed Stochastic Neighbor Embedding (t-SNE)\n","\n","t-SNE performs dimensionality reduction by modeling a random walk on the data set.  Specifically, $p_{ij}\\propto \\exp\\left[\\frac{-D_{ij}^2}{2\\sigma_i^2}\\right]$ is the probability of transitioning from data point $i$ to data point $j$ in the high-dimensional space, and $q_{ij}\\propto \\frac{1}{1+\\Delta^2_{ij}(Y)}$ is the same probability in the low-dimensional space.  t-SNE equates these probabilities through minimizing the cost function $J = D_{KL}(p||q) = \\sum_{ij} p_{ij} \\log\\frac{p_{ij}}{q_{ij}}$ (this is called the Kullback-Leibler divergence between $p$ and $q$).  \n","\n","Note how in the equation for $p_{ij}$, there is a term $\\sigma_i$.  This value sets the effective neighborhood around point $i$.  Because high-dimensional spaces often can have large variations in density due to sampling limitations and changes in the local dimensionality, we typically choose $\\sigma$ independently for each data point in order to keep the number of nearest neighbors roughly constant.  More precisely, we use a quantity call the **perplexity** to determine this. The perplexity equals $2^{\\mathcal{H}_i}$, where $\\mathcal{H}_i = -\\sum_{j\\ne i}p_{ij}\\log{p_{ij}}$ is the entropy of the conditional probability distribution of transitions from point $i$.  It can be shown (and we will) that this quantity can be thought of as an effective number of neighbors."]},{"cell_type":"code","metadata":{"id":"UAsQpphoKZPo"},"source":["#Initializing the TSNE object. n_components is the dimensionality of the embedding \n","#and perplexity is, well, the perplexity (i.e., an effective number of neighbors).\n","#For the method you can choose either 'exact' or 'barnes-hut'. Barnes-Hut is faster, \n","#but less accurate\n","tsne_example = TSNE(n_components=2,perplexity=20,method='exact') \n","\n","#Fits the model and returns back the low-dimensional projections\n","Y = tsne_example.fit_transform(X)\n","\n","#To return the embedded points\n","Y2 = tsne_example.embedding_\n","\n","#Returns back the LLE error (the cost function value at the found minimum)\n","tsne_error = tsne_example.kl_divergence_"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ML_3Q20Gg78I"},"source":["###Uniform Manifold Approximation and Projection (UMAP)"]},{"cell_type":"markdown","metadata":{"id":"vDyc9Bx1OXKB"},"source":["The last method we will discuss today is UMAP.  As discussed in class, UMAP is philosophically similar to t-SNE, but has some nice computational properties (i.e., it's faster and takes up less memory), although it is not quite as focused soley on local structure like t-SNE is.  There are a large number of potential options for this method, which are fully itemized [here](https://umap-learn.readthedocs.io/en/latest/api.html)."]},{"cell_type":"code","metadata":{"id":"_dg6oU9JKZxm"},"source":["#Initializing the UMAP object. n_components is the dimensionality of the embedding, \n","#n_neighbors is the number of nearest neighbors to use, min_dist is the minimum \n","#distance between points in the embedded space (think of it as a repulsion term).\n","#If desired, it is also possible to input in a difference distance metric via 'metric'\n","umap_example = umap.UMAP(n_components=2,n_neighbors=15,min_dist=.1) \n","\n","#Fits the model and returns back the low-dimensional projections\n","Y = umap_example.fit_transform(X)\n","\n","#To embed new points into the UMAP manifold\n","Z = umap_example.transform(X2)\n","\n","#To return the embedded points\n","Y2 = umap_example.embedding_"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iKRuXEiFBjnH"},"source":["##Applying Dimensionality Reduction Techniques to MNIST"]},{"cell_type":"markdown","metadata":{"id":"RDiEkDz5D2tQ"},"source":["In this section, you will apply PCA, NMF, MDS, Isomap, LLE, t-SNE, and UMAP to a subset of the MNIST data set from last time (it would take too long to analyze all 70,000 digits for some of the methods)."]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"jfN4I44yChsL"},"source":["#Import MNIST Data\n","from sklearn.datasets import fetch_openml\n","mnist = fetch_openml(name='mnist_784')\n","allDigitData = mnist.data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IGaLLNHWSoeg"},"source":["#Select 1000 random digits\n","N = 1000\n","idx = random.choice(range(np.shape(allDigitData)[0]),N)\n","digitData = allDigitData[idx,:]\n","digitNames = mnist.target.astype(np.int)[idx]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"8HzmmBzypmWo"},"source":["# @title Helper Functions (Run this code)\n","\n","def plot_MNIST_reconstruction(X, X_reconstructed):\n","  \"\"\"\n","  Plots 9 images in the MNIST dataset side-by-side with the reconstructed\n","  images.\n","\n","  Args:\n","    X (numpy array of floats)               : Data matrix each column\n","                                              corresponds to a different\n","                                              random variable\n","    X_reconstructed (numpy array of floats) : Data matrix each column\n","                                              corresponds to a different\n","                                              random variable\n","\n","  Returns:\n","    Nothing.\n","  \"\"\"\n","\n","  plt.figure()\n","  ax = plt.subplot(121)\n","  k = 0\n","  for k1 in range(3):\n","    for k2 in range(3):\n","      k = k + 1\n","      plt.imshow(np.reshape(X[k, :], (28, 28)),\n","                 extent=[(k1 + 1) * 28, k1 * 28, (k2 + 1) * 28, k2 * 28],\n","                 vmin=0, vmax=255)\n","  plt.xlim((3 * 28, 0))\n","  plt.ylim((3 * 28, 0))\n","  plt.tick_params(axis='both', which='both', bottom=False, top=False,\n","                  labelbottom=False)\n","  ax.set_xticks([])\n","  ax.set_yticks([])\n","  plt.title('Data')\n","  plt.clim([0, 250])\n","  ax = plt.subplot(122)\n","  k = 0\n","  for k1 in range(3):\n","    for k2 in range(3):\n","      k = k + 1\n","      plt.imshow(np.reshape(np.real(X_reconstructed[k, :]), (28, 28)),\n","                 extent=[(k1 + 1) * 28, k1 * 28, (k2 + 1) * 28, k2 * 28],\n","                 vmin=0, vmax=255)\n","  plt.xlim((3 * 28, 0))\n","  plt.ylim((3 * 28, 0))\n","  plt.tick_params(axis='both', which='both', bottom=False, top=False,\n","                  labelbottom=False)\n","  ax.set_xticks([])\n","  ax.set_yticks([])\n","  plt.clim([0, 250])\n","  plt.title('Reconstructed')\n","  plt.tight_layout()\n","\n","\n","def plot_MNIST_sample(X):\n","  \"\"\"\n","  Plots 9 images in the MNIST dataset.\n","\n","  Args:\n","     X (numpy array of floats) : Data matrix each column corresponds to a\n","                                 different random variable\n","\n","  Returns:\n","    Nothing.\n","\n","  \"\"\"\n","\n","  fig, ax = plt.subplots()\n","  k = 0\n","  for k1 in range(10):\n","    for k2 in range(10):\n","      k = k + 1\n","      plt.imshow(np.reshape(X[k, :], (28, 28)),\n","                 extent=[(k1 + 1) * 28, k1 * 28, (k2+1) * 28, k2 * 28],\n","                 vmin=0, vmax=255)\n","  plt.xlim((10 * 28, 0))\n","  plt.ylim((10 * 28, 0))\n","  plt.tick_params(axis='both', which='both', bottom=False, top=False,\n","                  labelbottom=False)\n","  plt.clim([0, 255])\n","  ax.set_xticks([])\n","  ax.set_yticks([])\n","  plt.show()\n","\n","def plot_MNIST_weights(weights):\n","  \"\"\"\n","  Visualize PCA basis vector weights for MNIST. Red = positive weights,\n","  blue = negative weights, white = zero weight.\n","\n","  Args:\n","     weights (numpy array of floats) : PCA basis vector\n","\n","  Returns:\n","     Nothing.\n","  \"\"\"\n","\n","  #fig, ax = plt.subplots()\n","  cmap = plt.cm.get_cmap('seismic')\n","  Z = np.zeros((28*4,28*6))\n","  k = 0\n","  for i in range(4):\n","    xstart = 0 + i*28\n","    xend = (i+1)*28\n","    for j in range(6):\n","      ystart = 0 + j*28\n","      yend = (j+1)*28\n","      Z[xstart:xend,ystart:yend] = np.real(np.reshape(weights[k,:], (28, 28)))\n","      k += 1\n","  #plt.imshow(np.real(np.reshape(weights[i,:], (28, 28))), cmap=cmap)\n","  plt.imshow(Z,cmap=cmap)\n","  plt.tick_params(axis='both', which='both', bottom=False, top=False,\n","                  labelbottom=False)\n","  plt.clim(-.15, .15)\n","  plt.colorbar()\n","  #ax.set_xticks([])\n","  #ax.set_yticks([])\n","  plt.show()\n","\n","\n","def plot_MNIST_weights_NMF(weights):\n","  \"\"\"\n","  Visualize PCA basis vector weights for MNIST. Red = positive weights,\n","  blue = negative weights, white = zero weight.\n","\n","  Args:\n","     weights (numpy array of floats) : PCA basis vector\n","\n","  Returns:\n","     Nothing.\n","  \"\"\"\n","\n","  #fig, ax = plt.subplots()\n","  cmap = plt.cm.get_cmap('Greys')\n","  Z = np.zeros((28*4,28*6))\n","  k = 0\n","  for i in range(4):\n","    xstart = 0 + i*28\n","    xend = (i+1)*28\n","    for j in range(6):\n","      ystart = 0 + j*28\n","      yend = (j+1)*28\n","      Z[xstart:xend,ystart:yend] = np.real(np.reshape(weights[k,:], (28, 28)))\n","      k += 1\n","  #plt.imshow(np.real(np.reshape(weights[i,:], (28, 28))), cmap=cmap)\n","  plt.imshow(Z,cmap=cmap)\n","  plt.tick_params(axis='both', which='both', bottom=False, top=False,\n","                  labelbottom=False)\n","  plt.clim(0,.6*np.max(weights))\n","  plt.colorbar()\n","  #plt.colorbar(ticks=[-.15, -.1, -.05, 0, .05, .1, .15])\n","  #ax.set_xticks([])\n","  #ax.set_yticks([])\n","  plt.show()\n","\n","\n","def add_noise(X, frac_noisy_pixels):\n","  \"\"\"\n","  Randomly corrupts a fraction of the pixels by setting them to random values.\n","\n","  Args:\n","     X (numpy array of floats)  : Data matrix\n","     frac_noisy_pixels (scalar) : Fraction of noisy pixels\n","\n","  Returns:\n","     (numpy array of floats)    : Data matrix + noise\n","\n","  \"\"\"\n","\n","  X_noisy = np.reshape(X, (X.shape[0] * X.shape[1]))\n","  N_noise_ixs = int(X_noisy.shape[0] * frac_noisy_pixels)\n","  noise_ixs = np.random.choice(X_noisy.shape[0], size=N_noise_ixs,\n","                               replace=False)\n","  X_noisy[noise_ixs] = np.random.uniform(0, 255, noise_ixs.shape)\n","  X_noisy = np.reshape(X_noisy, (X.shape[0], X.shape[1]))\n","\n","  return X_noisy\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G7a7fP3oWJMt"},"source":["####<font color=\"blue\">Question #1: Run both PCA and NMF on ```digitData``` (let the NMF```n_components=2``` for now).  Plot the 2-D projections you obtain with this method (on different subplots).  What are the similarities and differences? </font>\n","\n","<font color=\"blue\"> (Note: Note, you can use:\n","\n","```plt.scatter(Y[:,0],Y[:,1],c=digitNames,edgecolor=\"none\",cmap=plt.cm.get_cmap('nipy_spectral'));plt.colorbar()```\n","\n","to create a scatter plots with the digits colored seperately) </font>"]},{"cell_type":"code","metadata":{"id":"-hRe37bwXAtU"},"source":["#Type code for Question #1 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rR4ecNvgXJ3E"},"source":["<font color=\"green\">Compare the 2-D projections you obtain with this method.  What are the similarities and differences? </font>\n"]},{"cell_type":"markdown","metadata":{"id":"FVBSRhIvYQvs"},"source":["####<font color=\"blue\">Question #2: Now run both PCA and NMF with ```n_components=28```.  \n","\n","Use: </font>\n","\n","```plt.rcParams[\"figure.figsize\"] = (20,10)```\n","\n","```plt.subplot(2,1,1)```\n","\n","```plot_MNIST_weights(```[Your PCA object]```.components_)```\n","\n","```plt.subplot(2,1,2)```\n","\n","```plot_MNIST_weights_NMF(```[Your NMF object]```.components_)```\n","\n","<font color=\"blue\">to plot the basis vectors for both PCA and NMF.  How do the basis vectors differ? What is your explanation for the difference?"]},{"cell_type":"code","metadata":{"id":"FFwzs2Eebgbe"},"source":["#Type code for Question #2 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JiqwN5aSbmXN"},"source":["<font color=\"green\"> How do the basis vectors differ? What is your explanation for the difference? </font>"]},{"cell_type":"markdown","metadata":{"id":"PQejHt98bz4i"},"source":["####<font color=\"blue\">Question #3: Apply t-SNE to the MNIST data (embedding into two dimensions), using perplexities of 2, 5, and 50 (remembering to re-initialize a new TSNE object each time) and make scatter plots of each of the resulting embeddings (using the code from Question #1 to plot). Describe how the results are changing as a function of the perplexity and explain why these changes are likely occuring.</font>"]},{"cell_type":"code","metadata":{"id":"sOZ_BahWc9_o"},"source":["#Type code for Question #3 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cw6dxFR-dDAD"},"source":["<font color=\"green\">Describe how the results are changing as a function of the perplexity and explain why these changes are likely occuring.</font>"]},{"cell_type":"markdown","metadata":{"id":"l232BDZxeK6C"},"source":["####<font color=\"blue\">Question #4: Now, apply all 7 dimensionality reduction techniques (PCA, NMF, Isomap, LLE, MDS, t-SNE, and UMAP) to these data and plot the resulting 2-D embeddings.  You goal is to try and get the best possible clustering structure from each of these methods, where each digit is distinctly seperated from other digits, so you might need to play with the embedding parameters (e.g., ```n_neighbors```, ```perplexity```, etc.) to get good results.  Which method(s) would you choose to best seperate out the digits?  Why?  Make sure to keep your favorite embedding, as we will be using it in the next section.</font>"]},{"cell_type":"code","metadata":{"id":"C9pJW3eDfM_7"},"source":["#Type code for Question #4 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sk93pUvRfIqv"},"source":["<font color=\"green\"> Which method(s) would you choose to best seperate out the digits?  Why? </font>"]},{"cell_type":"markdown","metadata":{"id":"DE6XSIeFfu0u"},"source":["##Clustering "]},{"cell_type":"code","metadata":{"id":"MGRRbOUnj-wn","cellView":"form"},"source":["# @title More Helper Functions\n","\n","\n","from matplotlib.patches import Ellipse\n","\n","def draw_ellipse(position, covariance, ax=None, **kwargs):\n","    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n","    ax = ax or plt.gca()\n","    \n","    # Convert covariance to principal axes\n","    if covariance.shape == (2, 2):\n","        U, s, Vt = np.linalg.svd(covariance)\n","        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n","        width, height = 2 * np.sqrt(s)\n","    else:\n","        angle = 0\n","        width, height = 2 * np.sqrt(covariance)\n","    \n","    # Draw the Ellipse\n","    for nsig in range(1, 4):\n","        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n","                             angle, **kwargs))\n","        \n","def plot_gmm(gmm, X, label=True, ax=None):\n","    ax = ax or plt.gca()\n","    labels = gmm.fit(X).predict(X)\n","    if label:\n","        ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n","    else:\n","        ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2)\n","    ax.axis('equal')\n","    \n","    w_factor = 0.2 / gmm.weights_.max()\n","    for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n","        draw_ellipse(pos, covar, alpha=w * w_factor)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UDqc_TeFf7Ut"},"source":["In this last section, we will apply k-means clustering and gaussian mixture models to your results from the previous section."]},{"cell_type":"markdown","metadata":{"id":"2n-hr3czgtWe"},"source":["####k-means clustering\n","\n","To apply k-means clustering, if you want to cluster data set ```Y``` into ```k``` clusters, the syntax is:"]},{"cell_type":"code","metadata":{"id":"4aRIOtULhHfQ"},"source":["km_example = KMeans(n_clusters=k)\n","km_example.fit(Y)\n","km_labels = km_example.predict(Y)\n","\n","#To plot:\n","plt.scatter(Y[:,0],Y[:,1],c=km_labels,cmap='Spectral');plt.colorbar();plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VbJTdPNUirfL"},"source":["####Gaussian mixture models (GMM)\n","\n","To apply a Gaussian Mixture Model, the syntax would be:"]},{"cell_type":"code","metadata":{"id":"1E-o0TFnjFN_"},"source":["gmm_example = GaussianMixture(n_components=k).fit(Y)\n","gmm_labels = gmm_example.predict(Y)\n","#Can plot in the same manner as above"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZP_1xCp2jeXF"},"source":["But because GMM contains a probabilistic model under the hood, it is also possible to find probabilistic cluster assignmentsâ€”in Scikit-Learn this is done using the ``predict_proba`` method.\n","This returns a matrix of size ``[n_samples, n_clusters]`` which measures the probability that any point belongs to the given cluster:"]},{"cell_type":"code","metadata":{"id":"z3wchy_3jiiT"},"source":["gmm_probs = gmm_example.predict_proba(Y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zrmJk2hpkZu0"},"source":["Also, to plot the \"probability ellipses\" around each gaussian, you can use this code:"]},{"cell_type":"code","metadata":{"id":"JNNUx4MgoWv6"},"source":["plot_gmm(gmm_example, Y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aEc9wEBkmAkN"},"source":["####<font color=\"blue\">Question #5: Apply k-means (k=10) clustering to your favorite 2-D embedding result from Question #4 and plot the results, coloring each point by its assigned cluster.  How well did this method do in identifying individual digits?  </font>"]},{"cell_type":"code","metadata":{"id":"akJjlA7emnpG"},"source":["#Type code for Question #5 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zP_THqFqmtRR"},"source":["<font color=\"green\">Question #5: How well did this method do in identifying individual digits?  </font>"]},{"cell_type":"markdown","metadata":{"id":"fRgl7c7Um0Kf"},"source":["####<font color=\"blue\">Question #6: Now apply k-means (k=10) clustering to the full data set (```digitData```) directly.  Plot the results (you can use your embedding for the x and y values on the scatterplot, but use these results for coloring the points).  How do the results compare to clustering in the low-d space? Why have they gotten better / worse / stayed the same?</font>"]},{"cell_type":"code","metadata":{"id":"cEv5yHtCnYpz"},"source":["#Type code for Question #6 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yG5ZVZa4njxi"},"source":["<font color=\"green\"> How do the results compare to clustering in the low-d space? Why have they gotten better / worse / stayed the same? </font>"]},{"cell_type":"markdown","metadata":{"id":"Z4Q9Cgs_n0Sh"},"source":["####<font color=\"blue\">Question #7: Now apply a Gaussian Mixture Model (10 components) to the low-dimensional data and plot the results using the ```plot_gmm``` code above.  How well did this method do compared to k-means?  Why do you think it did better / worse / the same?</font>"]},{"cell_type":"code","metadata":{"id":"18bgHaKxozmi"},"source":["#Type code for Question #7 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CGplO57EnxVo"},"source":["<font color=\"green\"> How well did this method do compared to k-means?  Why do you think it did better / worse / the same? </font>"]},{"cell_type":"markdown","metadata":{"id":"U5MGoCzeywax"},"source":["<font color=\"blue\"> Question #8: The mutual information is a measure of all of the non-linear correlations between two variables, and it is given by:\n","\\begin{equation}\n","I(X;Y) = \\sum_{x\\in X}\\sum_{y\\in Y} p(x,y)\\log\\frac{p(x,y)}{p(x)p(y)},\n","\\end{equation}\n","where $X$ and $Y$ are random variables (here, cluster assignments).  We will talk more about this quantity soon, but for now, if the information is larger, there is more correlation between the variables.  Thus, given that we know the answer from our clustering, the higher the value of the mutual information between our clustering and the true values (```digitNames```), the better.\n","\n","<font color=\"blue\"> Calculate the mutual information between ```digitNames``` and each of your three clusterings.  Do these values change your interpretations from the previous questions? (Note: you will have to use $0 \\log 0 \\approx 0$ to deal with cases where $p(x,y)=0$)\n","\n","<font color=\"blue\">\n","Note: to calculate $p(x,y)$, it might be helpful to use: \n","\n","```pxy,xedges,ydeges = np.histogram2d(data1,data2,bins=(np.arange(11),np.arange(11)))``` </font>"]},{"cell_type":"code","metadata":{"id":"epqL7LrI1jHz"},"source":["#Type code for Question #8 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o9cEyDoE1ou_"},"source":["<font color=\"green\">Do these values change your interpretations from the previous questions? </font>"]}]}