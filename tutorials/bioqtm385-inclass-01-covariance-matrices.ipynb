{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bioqtm385-inclass-01-covariance-matrices.ipynb","provenance":[],"authorship_tag":"ABX9TyOh1/GfNd55p4jamXXBMAi8"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"oUssrdsbejdX"},"source":["**BIO/QTM 385: In class exercise for Monday, August 30th (answers will be the vast majority of Assignment #1, Due 9/8)**"]},{"cell_type":"markdown","metadata":{"id":"Jsu1w-bxU6k0"},"source":["<font color='green'>**Enter your names and group numbers here.**  </font>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oKPPgX5sewF-"},"source":["For this exercise, you will familiarize yourself with covariance matrices (as well as a bit more python), learning how to calculate them, about some of their interesting properties, and how to interpret a covariance matrix that is inferred from data.  We will discuss all of this more in lecture on Wednesday (so don't worry if things don't make perfect sense yet or if you can't get through the whole exercise), but this will hopefully serve as an introduction to the topic.  To provide some orientation, I put made all questions to be answered in <font color='blue'>blue text</font>, and the spaces where you should put your answers are in <font color='green'>green text</font>.  Sean and I will be cycling through the breakout rooms to answer whatever questions may arise.\n","\n","Also note that although today's exercise is more abstract, we will start applying these ideas to real data sets over the following two class periods."]},{"cell_type":"markdown","metadata":{"id":"EtqnAHLYfn_g"},"source":["####To start, we will import ```numpy```, ```numpy.random```, ```numpy.linalg```, and ```matplotlib```."]},{"cell_type":"code","metadata":{"id":"XYeSr60yfP5X"},"source":["import numpy as np\n","from numpy import random\n","from numpy import linalg\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b4Y3WzCykOkv"},"source":["## The Covariance Matrix\n"]},{"cell_type":"markdown","metadata":{"id":"2Lt4g7Q6XLVX"},"source":["###Defining the Covariance matrix"]},{"cell_type":"markdown","metadata":{"id":"BVdGL3FMf9vu"},"source":["When dealing with high-dimensional data (especially real-valued data), one of the first things we will typically study is the covariance matrix, $C$.  This matrix descibes all of the pairwise linear correlations between the $d$ different variables that are simultaneously measured.  While this matrix has many limitations in terms of its ability to characterize the high-dimensional data (see question below), there is often much to be learned - and it's easy to calculate! - so it's usually a great place to orient one's self when staring at a new dataset.\n","\n","To be more precise, the covariance matrix is the collection of all linear covariances between each pair of variables.  Mathematically, if $X\\in\\Re^{N\\times d}$ ($N$ is the number of data points, $d$ is the dimensionality of the data), then we define the covariance matrix, C, via:\n","\\begin{equation}\n","C_{ij} = \\frac{1}{N}\\sum_{k=1}^N (X_{ki} - \\bar{X_i})(X_{kj} - \\bar{X_j}),\n","\\end{equation}\n","where \n","\\begin{equation}\n","\\bar{X_i} = \\frac{1}{N}\\sum_{k=1}^N X_{ki}\n","\\end{equation}\n","is the mean of the $i$th variable in $X$.  \n","\n","Note how if $i=j$, $C_{ii}$ is simply the variance of $X_i$.  Thus, the diagonal terms of the matrix describe how individual variables vary, and each of the off-diagonal terms is the *covariance* between the two associated variables (hence the name of the overall matrix).\n","\n","While each of the entries in the matrix can be positive (positively correlated), negative (negatively correlatied), or zero (uncorrelated), the covariance matrix is symmetric (switching $i$ and $j$ in the definition of $C_{ij}$ above results in the same result, so $C_{ij} = C_{ji}$).  Symmetric matrices have many convenient [properties](https://en.wikipedia.org/wiki/Symmetric_matrix), including the fact that all of its eigenvalues must be greater than zero.  We will use this property later.   "]},{"cell_type":"markdown","metadata":{"id":"kv8Riu0jhQ9z"},"source":["<font color='blue'>Question #1: Name at least three potential limitations of using the covariance matrix to characterize high-dimensional data</font> "]},{"cell_type":"markdown","metadata":{"id":"4d8ANERUqtWe"},"source":["<font color='green'>Type answer here</font>"]},{"cell_type":"markdown","metadata":{"id":"doJi0WtHXQ_z"},"source":["### Calculating the Covariance matrix"]},{"cell_type":"markdown","metadata":{"id":"bcw6BzHpqnWk"},"source":["<font color='blue'> Question #2: Given that ```X``` is an ```N``` x ```d``` ```numpy``` array of real valued numbers, write a short script in the box below that (i) initializes a ```d``` x ```d``` ```numpy``` array, ```C``` and (ii) uses two ```for``` loops to fill in each element in ```C``` with the appropriate covariance matrix value, as defined above.</font>"]},{"cell_type":"code","metadata":{"id":"pqLRiSxWsOMp"},"source":["#write answer to question #2 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-3jVjCCbwz4x"},"source":["Despite the question above, you typically will calculate this matrix in ```numpy``` using [```np.cov()```](https://numpy.org/doc/stable/reference/generated/numpy.cov.html).  Note, however, that this function assumes that the input vector takes the form of a dimensions) by the # of data points matrix (i.e., $d\\times N$ rather than $N\\times d$), so if your data set is in the latter format, you will need to transpose it first (e.g., ```C = np.cov(X.T)```)."]},{"cell_type":"markdown","metadata":{"id":"mUvgEyMuxvCo"},"source":["<font color='blue'> Question #3: In the box below, generate a 1000 x 5 matrix, ```Y```, of gaussian random numbers ($mu=0$ and $\\sigma=3$). Then calculate and display its 5 x 5 covariance matrix.  Do the off-diagonal and diagonal entries match your expectations? Why or why not? (You can go back to the python tutorial for a refresher on generating random numbers)</font>"]},{"cell_type":"code","metadata":{"id":"D4d_1wKyzaPJ"},"source":["#write answer to question #3 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pkw87DYlzdth"},"source":["<font color='green'>Do the off-diagonal and diagonal entries match your expectations?  Why or why not?</font> \n","\n"]},{"cell_type":"markdown","metadata":{"id":"B0WNkHHqXsRW"},"source":["###Eigenvalues and eigenvectors of the covariance matrix"]},{"cell_type":"markdown","metadata":{"id":"zSA1YpBtWyTP"},"source":["Lastly, an important means of characterizing the covariance matrix is through its eigenvectors ($\\hat{v}_1,\\hat{v}_2,\\ldots,\\hat{v}_d$) and eigenvalues ($\\lambda_1,\\lambda_2,\\ldots,\\lambda_d$).  Importantly, these quantities often will provide information about how multiple variables are (linearly) interacting.\n","\n","For those who haven't had linear algebra yet (or are in need of a brief review), eigenvectors and eigenvalues are pairs of scalar quantities (eigenvalues) and vectors (naturally, eigenvectors), that satisfy the equation:\n","\\begin{equation}\n","C \\hat{v}_i = \\lambda_i \\hat{v}_i,\n","\\end{equation}\n","where $C$ is the covariance matrix.\n","\n","For symmetric, real, matrices like the covariance matrix, the eigenvectors and eigenvalues obey the following properties (amongst others):\n","- Each of the eigenvectors are orthogonal to each other ($\\hat{v}_i\\cdot \\hat{v}_j = 0$ if $i\\ne j$)\n","- By convention, each eigenvector has a norm of 1 ($\\hat{v}_i \\cdot \\hat{v}_i = 1$).\n","- Unless any of the data columns generating $C$ are exact multiples of each other (highly unlikely for any real data), the eigenvectors will span $\\Re^d$.  In other words, any $d$-dimensional real vector can be written as a linear combination of the eigenvectors\n","- All of the eigenvalues must be greater than or equal to zero and strictly greater than zero unless any of the data columns generating $C$ are exact multiples of each other (again, a highly-unlikely thing to have happen).\n"]},{"cell_type":"markdown","metadata":{"id":"mGWPmeMJoApw"},"source":["More conceptually, we can think of the eigenvectors as the directions where most variance in the data lie.  More precisely, if we order the eigenvalues such that $\\lambda_1 \\ge \\lambda_2 \\ge \\ldots \\lambda_d > 0$, then, $\\hat{v}_1$ is the direction in the data set where we see the most variance, $\\hat{v}_2$ is the direction with the second-most variance, and so on.  Moreover, $\\lambda_i$ is the variance in the data when projecting it onto $\\hat{v}_i$.  Don't worry, however, if this doesn't quite click now.  We will discuss this property in detail on Wednesday when we introduce Principal Components Analysis (PCA)."]},{"cell_type":"markdown","metadata":{"id":"ZxuVEeSVvUPy"},"source":["####Calculating eigenvalues and eigenvectors in ```numpy```"]},{"cell_type":"markdown","metadata":{"id":"uRaHHYMzri-i"},"source":["To find the eigenvectors and values of a matrix in ```numpy```, you can use the ```linalg.eig()``` function.  Here, the syntax is \n","\n","```eigvals,eigvecs = linalg.eig(A) ```\n","\n","where ```A``` is the matrix you wish to find the eigenvalues of, ```eigvals``` is a $1\\times d$ vector of eigenvectors, and ```eigvecs``` is a $d\\times d$ matrix of eigenvectors (each column is associated with the corresponding eigenvalues -- e.g., ```eigvecs[:,0]``` is the eigenvalue pair of ```eigvals[0]```)."]},{"cell_type":"markdown","metadata":{"id":"DXiquJIWwpuM"},"source":["<font color='blue'> Question #4: Calculate and display the eigenvectors and eigenvalues for the matrix you generated in Question #3.  How do these findings compare to what you would expect for a matrix with random entries? </font>"]},{"cell_type":"code","metadata":{"id":"7FrtxmYrwpRu"},"source":["#write your answer to question #4 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rP0_UAch0ILB"},"source":["<font color = \"green\">  How do these findings compare to what you would expect for a matrix with random entries?</font>"]},{"cell_type":"markdown","metadata":{"id":"i9IS_F8t1Vac"},"source":["As a note, it is often useful to sort the eigenvalues/vectors according to the eigenvalues.  To achieve this, you can use the following script:"]},{"cell_type":"code","metadata":{"id":"fjRogMXI1Up7"},"source":["idx = eigvals.argsort()[::-1]\n","eigvals = eigvals[idx]\n","eigvecs = eigvecs[:,idx]\n","print(eigvals)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qeCW7q6mVWPH"},"source":["## Measuring the covariance matrix"]},{"cell_type":"markdown","metadata":{"id":"XUOC-L6KVffi"},"source":["Like any other empirically derived quantity, the covariance matrix is a quantity that we estimate from data.  Thus, we are often tasked with the job of figuring-out what aspects of the apparent structure in our measured covariance matrix are \"real\" and what aspects could be generated by statistical fluctuations.  In this section, we will investigate aspects of this problem, but we will return to this topic (in a more formal manner) later in the course."]},{"cell_type":"markdown","metadata":{"id":"7aiRnNaFPVi7"},"source":["###Calculating the covariance matrix from samples"]},{"cell_type":"markdown","metadata":{"id":"aQiih0JQ52zb"},"source":["For this section, we will start by looking at samples from data drawn from a 3-D gaussian (normal) distribution with a mean of zero ($\\vec{\\mu} = \\left(\\begin{matrix}\n","0 \\\\\n","0 \\\\ \n","0\n","\\end{matrix}\\right)$) and a covariance matrix, $C = \\left(\\begin{matrix}\n","9 & 4 & -2\\\\\n","4 & 4 & 1/10 \\\\\n","-2 & 1/10 & 1\n","\\end{matrix}\\right)$.  The code in the cell below will generate and plot a data set of 100 random samples from the distribution."]},{"cell_type":"code","metadata":{"id":"3KaM9hG34Vwb"},"source":["#initialize parameters\n","N = 100\n","cov_matrix = [[9,4,-2],[4,4,.1],[-2,.1,1]]\n","mean_data = [0,0,0]\n","\n","#draw N random samples from the gaussian\n","data = random.multivariate_normal(mean_data,cov_matrix,size=N)\n","\n","#plotting each column against each other column\n","fig = plt.figure(figsize=(12,12))\n","plt.subplot(3,3,2)\n","plt.plot(data[:,0],data[:,1],'.')\n","plt.title('$x_0$ vs. $x_1$')\n","\n","plt.subplot(3,3,3)\n","plt.plot(data[:,0],data[:,2],'.')\n","plt.title('$x_0$ vs. $x_2$')\n","\n","plt.subplot(3,3,4)\n","plt.plot(data[:,1],data[:,0],'.')\n","plt.title('$x_1$ vs. $x_0$')\n","\n","plt.subplot(3,3,6)\n","plt.plot(data[:,1],data[:,2],'.')\n","plt.title('$x_1$ vs. $x_2$')\n","\n","plt.subplot(3,3,7)\n","plt.plot(data[:,2],data[:,0],'.')\n","plt.title('$x_2$ vs. $x_0$')\n","\n","plt.subplot(3,3,8)\n","plt.plot(data[:,2],data[:,1],'.')\n","plt.title('$x_2$ vs. $x_1$')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hueAi8er7lDk"},"source":["<font color='blue'> Question #5: Calculate and print the mean and the standarde error of the mean (s.e.m. = $\\sigma/\\sqrt{N-1}$) for the samples you just generated.  Is your value for the $\\mu$ within error of zero? (Note: to take the mean along a column of data, you need to use ```np.mean(data,axis=0)```)</font>"]},{"cell_type":"code","metadata":{"id":"ngp2JFhJ4sDZ"},"source":["#type your answer to Question #5 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wsRmdPBZ-SN_"},"source":["<font color = \"green\">  Is your value for the  ùúá  within error of zero?</font>"]},{"cell_type":"markdown","metadata":{"id":"j-Re3Kzo9yxv"},"source":["<font color='blue'> Question #6: Calculate and print the covariance matrix for the same samples as in Question #5.  How does it compate to the actual covariance matrix?</font>"]},{"cell_type":"code","metadata":{"id":"VYdt64S1-_4i"},"source":["#type your answer to Question #6 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nfW6zjPW9Xji"},"source":["<font color = \"green\">  How does it compate to the actual covariance matrix?</font>"]},{"cell_type":"markdown","metadata":{"id":"R2Pa487M_Ohp"},"source":["###Distributions of sampled covariance matrices"]},{"cell_type":"markdown","metadata":{"id":"kYyRnaMh_gM1"},"source":["This, of course, is just one simulated data set.  When trying to assess how having a finite sampling size (and all data sets are of finite size!) affects our results."]},{"cell_type":"markdown","metadata":{"id":"TOCLYyS8H3sw"},"source":["The code below will calculate the means, covariances matrices, and eigenvalues (sorted from largest to smallest) for 20,000 different instantiations of the 3-D gaussian described above.  ```means``` is a 20,000 x 3 matrix of the means from each instantiation, ```cov_matrices``` is a 20,000 x 3 x 3 array of covariance matrices, and ```eigenvalues`` is a 20,000 x 3 array of eigenvalues, arranged from largest to smallest."]},{"cell_type":"code","metadata":{"id":"4ISSI_jbH42k"},"source":["numDataSets = 20000\n","N = 100\n","cov_matrix = [[9,4,-2],[4,4,.1],[-2,.1,1]]\n","mean_data = [0,0,0]\n","\n","cov_matrices = np.zeros((numDataSets,3,3))\n","means = np.zeros((numDataSets,3))\n","eigenvalues = np.zeros((numDataSets,3))\n","for i in range(numDataSets):\n","    temp_data = random.multivariate_normal(mean_data,cov_matrix,size=N)\n","    cov_matrices[i,:,:] = np.cov(temp_data.T)\n","    means[i,:] = np.mean(temp_data,axis=0)\n","    eigvals,eigvecs = linalg.eig(cov_matrices[i,:,:])\n","    eigvals[::-1].sort()\n","    eigenvalues[i,:] = eigvals\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hxzpeNycSFXy"},"source":["<font color='blue'> Question #7: Using the results from the code above, make 50-bin histograms for each of the nine covariance matrix entries (you can use the same ```plt.subplot()``` code as shown above).  Do the actual values lie within the found distributions?</font>"]},{"cell_type":"code","metadata":{"id":"P4GqWUR-SEif"},"source":["#type your answer to Question #7 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5xd89HknT01z"},"source":["<font color='green'> Do the actual values lie within the found distributions?</font>"]},{"cell_type":"markdown","metadata":{"id":"sXQYhHpnTh5x"},"source":["<font color='blue'> Question #8: Using the results from the code above, make 50-bin histograms for each of the three covariance matrix eigenvalues.  Do the distributions look gaussian, or are they asymmetric?</font>\n"]},{"cell_type":"code","metadata":{"id":"QNLtlUXSUQ3W"},"source":["#type your answer to Question #8 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"StdlxrTGUR7M"},"source":["<font color='green'> Do the distributions look gaussian, or are they asymmetric? </font>"]},{"cell_type":"markdown","metadata":{"id":"ipRz5gtqWej8"},"source":["###Assessing statistical significance for eigenvalues"]},{"cell_type":"markdown","metadata":{"id":"A881C-MzWw2U"},"source":["One way of performing *dimensionality reduction* (i.e., translating a high-dimensional representation into a lower-dimensional one) is to project a data set onto the space of eigenvectors corresponding to eigenvalues that are \"significantly\" different from zero.  Below, we will see one way of performing this assessment via independently shuffling the columns of a data matrix.\n","\n","(Editorial note: I put \"significantly\" in scare quotes here, as I don't really like the term -- it's really just a way to pretend that an arbitrary threshold is really a magic value that delineates success from failure.  OK, rant over.  For now.)"]},{"cell_type":"markdown","metadata":{"id":"MB3octCBZHeV"},"source":["Let's go back again to our 3-D gaussian matrix from before."]},{"cell_type":"code","metadata":{"id":"UVkiZ-ytWCc-"},"source":["#initialize parameters\n","N = 100\n","cov_matrix = [[9,4,-2],[4,4,.1],[-2,.1,1]]\n","mean_data = [0,0,0]\n","\n","#draw N random samples from the gaussian\n","data = random.multivariate_normal(mean_data,cov_matrix,size=N)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0mZEzbevZlIz"},"source":["Because the covariance matrix has off-diagonal terms, the are correlations between the three measured variables.  A natural question to ask, though, is: *if there were no correlations, might we still see some by accident just because we've only measured a finite number of data points?*\n","\n","One way to answer this question is to shuffle each of the columns independently from one another.  Thus, the resulting matrix **should** have no correlations, but, of course, will display some correlations due to noise from finite sampling.  We then will measure the resulting eigenvalues from these shuffled matrices, repeat the process multiple times, and will compare the resulting distribution of eigenvalues from shuffled matrices to the eigenvalues from the measured samples."]},{"cell_type":"markdown","metadata":{"id":"fhh5rBfLckHR"},"source":["First, to shuffle the columns in the matrix, we can use the following code:"]},{"cell_type":"code","metadata":{"id":"1Qy-X21tWE6m"},"source":["shuffled_data = np.copy(data)\n","#note: we can't say \"shuffled_data = data\" like in matlab or R, because then, \n","#the created variable would be linked to the old variable\n","for i in range(3):\n","  random.shuffle(shuffled_data[:,i])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gh8pnt4Yfrn9"},"source":["<font color='blue'> Question #9: Calculate the covariance matrix eigenvalues (sorted from largest to smallest) from the ```data``` matrix above.</font>\n"]},{"cell_type":"code","metadata":{"id":"hHuusgo1f-YQ"},"source":["#type your answer to Question #9 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Yjsvr6KgEDk"},"source":["<font color='blue'> Question #10: Perform the shuffling analysis described in the paragraph above on the ```data``` matrix (perform the shuffling 1,000 times.  Create 50-bin histograms for the each of the three eigenvalues (remember to sort!). </font>\n"]},{"cell_type":"code","metadata":{"id":"kO0cO1Oxg2bS"},"source":["#type your answer to Question #10 here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PVf7XZL-g89z"},"source":["<font color='blue'> Question #11: In the histograms above, are the resulting distributions symmetric or non-symmetric?  If the latter, is there a more definitive upper-bound or a more definitive lower-bound?</font>\n"]},{"cell_type":"markdown","metadata":{"id":"PJIA6VBLhZ8P"},"source":["<font color='green'> Type your answer to Question #11 here </font>"]},{"cell_type":"markdown","metadata":{"id":"Aq4UbwiVhjnB"},"source":["<font color='blue'> Question #12: Given your answers to Questions #9, 10, and 11, which of the three eigenvalues that you've measured from ```data``` would you consider as \"signficant\" ? Explain your reasoning.</font>\n"]},{"cell_type":"markdown","metadata":{"id":"UhaVMN0JiGVV"},"source":["<font color='green'> Type your answer to Question #12 here </font>\n"]}]}